{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read analysis data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analysis_data(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(FILE_PATH_ANALYSIS_DATA, index_col=[\"option_name\", \"index\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(df: pd.DataFrame, parameter: str, metric: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    X = df.loc[(parameter), metric].to_numpy().reshape(-1,1)\n",
    "    y = df.loc[(parameter), \"option_value\"].to_numpy()\n",
    "    return X,y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_ANALYSIS_DATA = '../data/processed/analysis_LOM_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_analysis_data(FILE_PATH_ANALYSIS_DATA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Any, path: str):\n",
    "    \"\"\"Save model\n",
    "\n",
    "    Args:\n",
    "        model (Any): Scikit-learn model\n",
    "        path (str): Path to model\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path: str) -> Any:\n",
    "    \"\"\"Load model\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to model\n",
    "\n",
    "    Returns:\n",
    "        Any: Scikit-learn model\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    with open(path, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate seeds for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 100\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "seeds = rng.integers(low=0, high=1000, size=SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(seeds: np.ndarray=None, test_size: float=0.20, X: np.ndarray=None, y: np.ndarray=None) -> dict[str, Any]:\n",
    "    data = {\n",
    "        \"trials\": {},\n",
    "        \"best_seed\": -1,\n",
    "        \"best_estimator\": None,\n",
    "    }\n",
    "\n",
    "    max_train_score = 0\n",
    "\n",
    "    for seed in seeds:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "        clf2 = Ridge()\n",
    "        # Create a pipeline for polynomial regression\n",
    "        pipe2 = Pipeline([('standardscaler', StandardScaler()),('polynomialfeatures', PolynomialFeatures()), ('classifier', clf2)])\n",
    "\n",
    "        # Set up the parameter grid for the polynomial degree and regularization strength\n",
    "        params2 = {}\n",
    "        params2['polynomialfeatures__degree'] = np.arange(1,10)\n",
    "        params2['classifier__alpha'] = [0.1, 1, 10, 100]\n",
    "        params2['classifier'] = [clf2]\n",
    "\n",
    "        # get the indices that would sort array \n",
    "        indices = np.argsort(X_train, axis=0).flatten()\n",
    "\n",
    "        # use the indices to sort both arrays\n",
    "        X_sorted = X_train[indices]\n",
    "        y_sorted = y_train[indices].ravel()\n",
    "\n",
    "        # create a list of parameter dictionaries\n",
    "        params = [params2]\n",
    "\n",
    "        grid1 = GridSearchCV(pipe2, params)\n",
    "        grid1.fit(X_sorted, y_sorted)\n",
    "\n",
    "        # Store trial dadta\n",
    "        data[\"trials\"][seed] = {}\n",
    "        data[\"trials\"][seed][\"best_params\"] = grid1.best_params_\n",
    "        data[\"trials\"][seed][\"training_score\"] = grid1.best_score_\n",
    "        data[\"trials\"][seed][\"test_score\"] = grid1.best_estimator_.score(X_test, y_test)\n",
    "        \n",
    "        # Store best estimator\n",
    "        if grid1.best_score_ > max_train_score:\n",
    "            max_train_score = grid1.best_score_\n",
    "            data[\"best_seed\"] = seed\n",
    "            data[\"best_estimator\"] = grid1.best_estimator_\n",
    "\n",
    "    test_scores = [data[\"trials\"][seed][\"test_score\"] for seed in seeds]\n",
    "    print(f\"Test set R2 scores: {test_scores}\")\n",
    "    print(f\"Mean R2 test score: {np.mean(test_scores)}\")\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model to predict pad_gap from qubit frequency (fQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set R2 scores: [0.988201345811011, 0.9895452478292662, 0.9939378022942692, 0.9904382206901483, 0.9952910315398218, 0.8416812870477892, 0.9942691004436393, 0.9747087740388316, -1.1476255547633438, 0.9914734299427368, 0.9963791102693191, 0.9987139076725652, 0.9904686530391316, 0.9948215918745921, 0.9935189142817924, 0.9948654052291728, 0.9443175414871916, 0.8382122445602831, 0.9603377870178328, 0.9952678735630296, 0.9586754749673974, 0.9836864407223204, 0.9443019770186416, 0.9915408148430273, 0.9616731482188978, 0.9979735980260996, 0.9469810410193361, 0.9795246426464441, 0.9952938138071374, 0.9889072532903675, 0.9952678735630296, 0.9975897322326752, 0.9979572272135585, 0.9888792658100548, 0.9886250647106624, 0.9944446164791751, 0.8416812870477892, 0.9926599866098841, 0.9855776407222406, 0.9871768894466907, 0.9893026562990124, 0.9993785310478787, 0.9613330888425711, 0.9968291702812545, 0.9938750970340621, 0.9938998183631316, 0.9407664862231366, 0.9303681150432158, 0.9790181348641253, 0.9902079082517017, 0.9915853749276297, 0.9652205403177769, 0.9917513416255456, 0.993665695073093, 0.5965287993094541, 0.9959617949130083, 0.9635929447159668, 0.7790009045808849, 0.9031023120908714, 0.9585924051685573, 0.9914531869752112, 0.9902139371208332, 0.9886123666091534, 0.9982606304104648, 0.9861126585385238, 0.9995455571356975, 0.9978553976341609, 0.9836864407223204, 0.9928865447012126, 0.9949686281731162, 0.9933666058445054, 0.9974446487368681, 0.9969804268433717, 0.9971428894787941, 0.9960646124688409, 0.9849932512551309, 0.8696087330187807, 0.9991774943297949, 0.9901647778624203, 0.9662621175480376, 0.9631469954760798, 0.9516752995846687, 0.9886654298137211, 0.990154334105482, 0.993371817473773, 0.9613330888425711, 0.7376987349964115, 0.9836414419373212, 0.9744048278267232, 0.990154334105482, 0.9775902039220369, 0.9910681673549396, 0.5905769351359922, 0.9971329930716984, 0.9949265315024123, 0.9621859208731961, 0.995257127362556, 0.9958274272011636, 0.9922782481676425, 0.9676782613085798]\n",
      "Mean R2 test score: 0.9446479164670705\n"
     ]
    }
   ],
   "source": [
    "X, y = get_X_y(df, parameter=\"pad_gap_in_um\", metric=\"fQ\")\n",
    "data = train(seeds=seeds, X=X, y=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
